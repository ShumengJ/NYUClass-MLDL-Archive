{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShumengJ/ECEGY6143-ML-Archive/blob/main/7_hw_grid_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaBTllEjAnwG"
      },
      "source": [
        "# Homework: Grid search for hyperparameter tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epwEZzz3GuLG"
      },
      "source": [
        "* Name:\n",
        "* Net ID:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjJSZsD8G3RO"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw8FOjmiPWlJ"
      },
      "source": [
        "For models with a single hyperparameter controlling bias-variance (for example: $k$ in $k$ nearest neighbors), we used Scikit-learn's `KFoldCV` to test a range of values for the hyperparameter, and to select the best one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LU9WE49PX5S"
      },
      "source": [
        "When we have *multiple* hyperparameters to tune, we can use `GridSearchCV` to select the best *combination* of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XNw1p1ZBNoW"
      },
      "source": [
        "For example, in this week's lesson (in the notebook on bias and variance of SVM), we saw three ways to tune the bias-variance of an SVM classifier:\n",
        "\n",
        "* Changing the kernel\n",
        "* Changing $C$, the inverse of the regularization penalty weight\n",
        "* For an RBF kernel, changing $\\gamma$, the inverse of the kernel bandwidth\n",
        "\n",
        "\n",
        "To get the best performance from an SVM classifier, we need to find the best *combination* of these hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WGjwn5FCGof"
      },
      "source": [
        "This notebook shows how to use `GridSearchCV` to tune an SVM classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFS7vTtMXhF4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3drNBKX-J67W"
      },
      "source": [
        "## Get the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T0Xy6ClKMGA"
      },
      "source": [
        "We will work with a subset of the MNIST handwritten digits data. First, we will get the data, and assign a small subset of samples to training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9dNtVkiKBxh"
      },
      "source": [
        "from sklearn.datasets import fetch_openml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyErDOylKDb7"
      },
      "source": [
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsqROPqKJMp"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                     train_size=10000, test_size=3000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9op_RaxsKk8x"
      },
      "source": [
        "## Run grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY6NucORKoXQ"
      },
      "source": [
        "Then, we will define a *parameter grid* with all the combinations of hyperparameters that we want to test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ0xFE0f7wei"
      },
      "source": [
        "param_grid = [\n",
        "  {'C': [0.1, 1000], 'kernel': ['linear']},\n",
        "  {'C': [0.1, 1000], 'gamma': [0.01, 0.0001], 'kernel': ['rbf']},\n",
        " ]\n",
        "param_grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjB2Sg6qEU7T"
      },
      "source": [
        "We will pass the parameter grid to a `GridSearchCV`, along with the number of CV folds to use.\n",
        "\n",
        "Also, we set:\n",
        "\n",
        "* `verbose` to a large positive number, so that we get plenty of logging output, and\n",
        "* `refit` to `True`, so that after testing all of the hyperparameter combinations, it will re-fit an SVM classifier with the hyperparameters that had the best mean validation score.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pqQcCL2AFGd"
      },
      "source": [
        "clf = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lIRPXQaKzcH"
      },
      "source": [
        "## Review results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXcQs30KEsKr"
      },
      "source": [
        "Finally, we'll print the results of the cross validation. For each combination of parameters, we can see:\n",
        "\n",
        "* the validation score for each fold\n",
        "* the mean validation score\n",
        "* the standard deviation of the validation score\n",
        "* the rank, by mean validation score\n",
        "\n",
        "(in the report, the \"test\" scores are validation scores.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCnPOXUMAmNf"
      },
      "source": [
        "pd.DataFrame(clf.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE7g2RoYK2er"
      },
      "source": [
        "## Evaluate performance of the re-fitted model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAUpvYNUFY1g"
      },
      "source": [
        "We can see the \"best\" parameters, with which the model was re-fitted:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4b7icFQDHPw"
      },
      "source": [
        "print(clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BABJex6EFcNV"
      },
      "source": [
        "And we can evaluate the re-fitted model on the test set. (Note that the `GridSearchCV` only used the training set; we have not used the test set at all for model fitting.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu30afxMDLXv"
      },
      "source": [
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBbo9eNTDOcs"
      },
      "source": [
        "accuracy_score(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48g04_Z5LFNP"
      },
      "source": [
        "## Assignment\n",
        "\n",
        "The results of a `GridSearchCV` are only as good as the combinations of hyperparameters we test in the grid.\n",
        "\n",
        "* If the range of hyperparameter values is too narrow (it excludes good values), the model accuracy will be lower that it would be with a better choice of hyperparameters.\n",
        "* If the search space is large with a fine resolution, the grid search will take a very long time.\n",
        "* If the search space is large with a coarse resolution, we may not find a good combination of hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko5tsGcNFNnH"
      },
      "source": [
        "In the demo above, I did not use a good parameter grid. For your assignment, try to improve the parameter grid, and re-run the notebook with your modified parameter grid.\n",
        "\n",
        "Explain the results. In particular, explain: if *I* would run your notebook, with exactly the parameter grid you defined, would I be confident that the SVM performance is about as good as it can be? Why?\n",
        "\n",
        "Also answer the following question: suppose instead of using a `GridSearchCV`, I would separately run one `KFoldCV` over a range of values of $C$, one `KFoldCV` over a range of values of $\\gamma$, and one `KFoldCV` for two values of `kernel`. In other words, I would independently select a best value for each hyperparameter. Would this be a good strategy? Why or why not?\n",
        "\n",
        "Submit the PDF version of the notebook, including your explanation.\n",
        "\n"
      ]
    }
  ]
}