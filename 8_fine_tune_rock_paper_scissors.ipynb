{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShumengJ/ECEGY6143-ML-Archive/blob/main/8_fine_tune_rock_paper_scissors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yz-QM1sLt0C"
      },
      "source": [
        "# Demo: Transfer learning\n",
        "\n",
        "_Fraida Fund_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "428VGFYErPzg"
      },
      "source": [
        "In practice, for most machine learning problems, you wouldn't design or train a convolutional neural network from scratch - you would use an existing model that suits your needs (does well on ImageNet, size is right) and fine-tune it on your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVkYelEYxBrD"
      },
      "source": [
        "Note: for faster training, use Runtime > Change Runtime Type to run this notebook on a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoT5RmjYkt4J"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWnwUtAmLt1B"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import platform\n",
        "import datetime\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "\n",
        "print('Python version:', platform.python_version())\n",
        "print('Tensorflow version:', tf.__version__)\n",
        "print('Keras version:', tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zf6GEdiFgpZ"
      },
      "source": [
        "## Import data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T9eWRPrHmLO"
      },
      "source": [
        "The \"rock paper scissors\" dataset is available directly from the Tensorflow package. In the cells that follow, we'll get the data, plot a few examples, and also do some preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRYp7tP7HlkK"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFu9rKUAHrfg"
      },
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'rock_paper_scissors',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    with_info=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0AKQkqyHxQw"
      },
      "source": [
        "fig = tfds.show_examples(ds_info, ds_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWqxQT9yIKci"
      },
      "source": [
        "classes = np.array(['rock', 'paper', 'scissors'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu8XjI9iITAD"
      },
      "source": [
        "## Pre-process dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVLvPemAJPHG"
      },
      "source": [
        "INPUT_IMG_SIZE = 224\n",
        "INPUT_IMG_SHAPE = (224, 224, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSAiAgnkIUTD"
      },
      "source": [
        "def preprocess_image(sample):\n",
        "    sample['image'] = tf.cast(sample['image'], tf.float32)\n",
        "    sample['image'] = sample['image'] / 255.\n",
        "    sample['image'] = tf.image.resize(sample['image'], [INPUT_IMG_SIZE, INPUT_IMG_SIZE])\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_p47BorIfT_"
      },
      "source": [
        "ds_train = ds_train.map(preprocess_image)\n",
        "ds_test  = ds_test.map(preprocess_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqNf_Pl_KFq3"
      },
      "source": [
        "fig = tfds.show_examples(ds_train, ds_info, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKqVjwteQSQ2"
      },
      "source": [
        "We'll convert to `numpy` format again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ywewM5QT_e"
      },
      "source": [
        "train_numpy = np.vstack(tfds.as_numpy(ds_train))\n",
        "test_numpy = np.vstack(tfds.as_numpy(ds_test))\n",
        "\n",
        "X_train = np.array(list(map(lambda x: x[0]['image'], train_numpy)))\n",
        "y_train = np.array(list(map(lambda x: x[0]['label'], train_numpy)))\n",
        "\n",
        "X_test = np.array(list(map(lambda x: x[0]['image'], test_numpy)))\n",
        "y_test = np.array(list(map(lambda x: x[0]['label'], test_numpy)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LuXN9g5KymP"
      },
      "source": [
        "## Upload custom test sample\n",
        "\n",
        "This code expects a PNG image.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av46z0z1Z-My"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onHkdSeIK0Ut"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Edit the filename here as needed\n",
        "filename = 'scissors.png'\n",
        "\n",
        "# pre-process image\n",
        "image = Image.open(filename).convert('RGB')\n",
        "image_resized = image.resize((INPUT_IMG_SIZE, INPUT_IMG_SIZE), Image.BICUBIC)\n",
        "test_sample = np.array(image_resized)/255.0\n",
        "test_sample = test_sample.reshape(1, INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRu1KakPLIXw"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(4,4));\n",
        "plt.imshow(test_sample.reshape(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3));\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oCegdP1E8E6"
      },
      "source": [
        "## Classify with MobileNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLtalBb0OqrH"
      },
      "source": [
        "[Keras Applications](https://keras.io/api/applications/) are pre-trained models with saved weights, that you can download and use without any additional training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7EMIzoUOtR0"
      },
      "source": [
        "Here's a table of the models available as Keras Applications.\n",
        "\n",
        "In this table, the top-1 and top-5 accuracy refer to the model's performance on the ImageNet validation dataset, and depth is the depth of the network including activation layers, batch normalization layers, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYr0PHtrNPb4"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Model</th>\n",
        "<th align=\"right\">Size</th>\n",
        "<th align=\"right\">Top-1 Accuracy</th>\n",
        "<th align=\"right\">Top-5 Accuracy</th>\n",
        "<th align=\"right\">Parameters</th>\n",
        "<th align=\"right\">Depth</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>Xception</td>\n",
        "<td align=\"right\">88 MB</td>\n",
        "<td align=\"right\">0.790</td>\n",
        "<td align=\"right\">0.945</td>\n",
        "<td align=\"right\">22,910,480</td>\n",
        "<td align=\"right\">126</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>VGG16</td>\n",
        "<td align=\"right\">528 MB</td>\n",
        "<td align=\"right\">0.713</td>\n",
        "<td align=\"right\">0.901</td>\n",
        "<td align=\"right\">138,357,544</td>\n",
        "<td align=\"right\">23</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>VGG19</td>\n",
        "<td align=\"right\">549 MB</td>\n",
        "<td align=\"right\">0.713</td>\n",
        "<td align=\"right\">0.900</td>\n",
        "<td align=\"right\">143,667,240</td>\n",
        "<td align=\"right\">26</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ResNet50</td>\n",
        "<td align=\"right\">98 MB</td>\n",
        "<td align=\"right\">0.749</td>\n",
        "<td align=\"right\">0.921</td>\n",
        "<td align=\"right\">25,636,712</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ResNet101</td>\n",
        "<td align=\"right\">171 MB</td>\n",
        "<td align=\"right\">0.764</td>\n",
        "<td align=\"right\">0.928</td>\n",
        "<td align=\"right\">44,707,176</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ResNet152</td>\n",
        "<td align=\"right\">232 MB</td>\n",
        "<td align=\"right\">0.766</td>\n",
        "<td align=\"right\">0.931</td>\n",
        "<td align=\"right\">60,419,944</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ResNet50V2</td>\n",
        "<td align=\"right\">98 MB</td>\n",
        "<td align=\"right\">0.760</td>\n",
        "<td align=\"right\">0.930</td>\n",
        "<td align=\"right\">25,613,800</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ResNet101V2</td>\n",
        "<td align=\"right\">171 MB</td>\n",
        "<td align=\"right\">0.772</td>\n",
        "<td align=\"right\">0.938</td>\n",
        "<td align=\"right\">44,675,560</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ResNet152V2</td>\n",
        "<td align=\"right\">232 MB</td>\n",
        "<td align=\"right\">0.780</td>\n",
        "<td align=\"right\">0.942</td>\n",
        "<td align=\"right\">60,380,648</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>InceptionV3</td>\n",
        "<td align=\"right\">92 MB</td>\n",
        "<td align=\"right\">0.779</td>\n",
        "<td align=\"right\">0.937</td>\n",
        "<td align=\"right\">23,851,784</td>\n",
        "<td align=\"right\">159</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>InceptionResNetV2</td>\n",
        "<td align=\"right\">215 MB</td>\n",
        "<td align=\"right\">0.803</td>\n",
        "<td align=\"right\">0.953</td>\n",
        "<td align=\"right\">55,873,736</td>\n",
        "<td align=\"right\">572</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet</td>\n",
        "<td align=\"right\">16 MB</td>\n",
        "<td align=\"right\">0.704</td>\n",
        "<td align=\"right\">0.895</td>\n",
        "<td align=\"right\">4,253,864</td>\n",
        "<td align=\"right\">88</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNetV2</td>\n",
        "<td align=\"right\">14 MB</td>\n",
        "<td align=\"right\">0.713</td>\n",
        "<td align=\"right\">0.901</td>\n",
        "<td align=\"right\">3,538,984</td>\n",
        "<td align=\"right\">88</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>DenseNet121</td>\n",
        "<td align=\"right\">33 MB</td>\n",
        "<td align=\"right\">0.750</td>\n",
        "<td align=\"right\">0.923</td>\n",
        "<td align=\"right\">8,062,504</td>\n",
        "<td align=\"right\">121</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>DenseNet169</td>\n",
        "<td align=\"right\">57 MB</td>\n",
        "<td align=\"right\">0.762</td>\n",
        "<td align=\"right\">0.932</td>\n",
        "<td align=\"right\">14,307,880</td>\n",
        "<td align=\"right\">169</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>DenseNet201</td>\n",
        "<td align=\"right\">80 MB</td>\n",
        "<td align=\"right\">0.773</td>\n",
        "<td align=\"right\">0.936</td>\n",
        "<td align=\"right\">20,242,984</td>\n",
        "<td align=\"right\">201</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>NASNetMobile</td>\n",
        "<td align=\"right\">23 MB</td>\n",
        "<td align=\"right\">0.744</td>\n",
        "<td align=\"right\">0.919</td>\n",
        "<td align=\"right\">5,326,716</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>NASNetLarge</td>\n",
        "<td align=\"right\">343 MB</td>\n",
        "<td align=\"right\">0.825</td>\n",
        "<td align=\"right\">0.960</td>\n",
        "<td align=\"right\">88,949,818</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNetB0</td>\n",
        "<td align=\"right\">29 MB</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">5,330,571</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNetB1</td>\n",
        "<td align=\"right\">31 MB</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">7,856,239</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNetB2></td>\n",
        "<td align=\"right\">36 MB</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">9,177,569</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNetB3</td>\n",
        "<td align=\"right\">48 MB</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">12,320,535</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNetB4</td>\n",
        "<td align=\"right\">75 MB</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">19,466,823</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNetB5</td>\n",
        "<td align=\"right\">118 MB</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">30,562,527</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNetB6</td>\n",
        "<td align=\"right\">166 MB</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">43,265,143</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNetB7</td>\n",
        "<td align=\"right\">256 MB</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">-</td>\n",
        "<td align=\"right\">66,658,687</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmWhXI3Ar7rk"
      },
      "source": [
        "(A variety of other models is available from other sources - for example, the [Tensorflow Hub](https://tfhub.dev/).)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN03-oZBUbRc"
      },
      "source": [
        "I'm going to use MobileNetV2, which is designed specifically to be small and fast (so it can run on mobile devices!)\n",
        "\n",
        "MobileNets come in various sizes controlled by a multiplier for the depth (number of features), and trained for various sizes of input images. We will use the 224x224 input image size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Gso_YbFDCa"
      },
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "  input_shape=INPUT_IMG_SHAPE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc0bhqkKLOI1"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxSgZcURJkB9"
      },
      "source": [
        "base_probs = base_model.predict(test_sample)\n",
        "base_probs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4ow9zTYLaJr"
      },
      "source": [
        "url = tf.keras.utils.get_file(\n",
        "    'ImageNetLabels.txt',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet_classes = np.array(open(url).read().splitlines())[1:]\n",
        "imagenet_classes.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIo8NzaPUjQC"
      },
      "source": [
        "Let's see what the top 5 predicted classes are for my test image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0PUxOmWLuPH"
      },
      "source": [
        "most_likely_classes = np.argsort(base_probs.squeeze())[-5:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR21o8pZL1qT"
      },
      "source": [
        "plt.figure(figsize=(10,4));\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(test_sample.reshape(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3));\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "p = sns.barplot(x=imagenet_classes[most_likely_classes],y=base_probs.squeeze()[most_likely_classes]);\n",
        "plt.ylabel(\"Probability\");\n",
        "p.set_xticklabels(p.get_xticklabels(), rotation=45);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajm1cRRRViTd"
      },
      "source": [
        "MobileNetV2 is trained on a specific task: classifying the images in the ImageNet dataset by selecting the most appropriate of 1000 class labels.\n",
        "\n",
        "It is not trained for our specific task: classifying an image of a hand as rock, paper, or scissors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqMy89csyxiM"
      },
      "source": [
        "## Background: fine-tuning a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDyFCXjPyz7g"
      },
      "source": [
        "A typical convolutional neural network looks something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfeshFUoy3dc"
      },
      "source": [
        "![Image via [PlotNeuralNet](https://github.com/HarisIqbal88/PlotNeuralNet)](https://raw.githubusercontent.com/LongerVision/Resource/master/AI/Visualization/PlotNeuralNet/vgg16.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSrPHAAVzMX7"
      },
      "source": [
        "We have a sequence of convolutional layers followed by pooling layers. These layers are *feature extractors* that \"learn\" key features of our input images.\n",
        "\n",
        "Then, we have one or more fully connected layers followed by a fully connected layer with a softmax activation function. This part of the network is for *classification*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_70UN-NznN8"
      },
      "source": [
        "The key idea behind transfer learning is that the *feature extractor* part of the network can be re-used across different tasks and different domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbAT-kDJzyr-"
      },
      "source": [
        "This is especially useful when we don't have a lot of task-specific data. We can get a pre-trained feature extractor trained on a lot of data from another task, then train the classifier on task-specific data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtsb4tYQ0A1k"
      },
      "source": [
        "The general process is:\n",
        "\n",
        "* Get a pre-trained model, without the classification layer.\n",
        "* Freeze the base model.\n",
        "* Add a classification layer.\n",
        "* Train the model (only the weights in your classification layer will be updated).\n",
        "* (Optional) Un-freeze some of the last layers in your base model.\n",
        "* (Optional) Train the model again, with a smaller learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699crssOKhgf"
      },
      "source": [
        "## Train our own classification head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-u2euCjKvBP"
      },
      "source": [
        "This time, we will get the MobileNetV2 model *without* the fully connected layer at the top of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waK5a2Dlbc_X"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1sm4kCRKulg"
      },
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "  input_shape=INPUT_IMG_SHAPE,\n",
        "  include_top=False,\n",
        "  pooling='avg'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOAk4cWfLJr7"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBocHqAKK-yW"
      },
      "source": [
        "Then, we will *freeze* the model. We're not going to train the MobileNetV2 part of the model, we're just going to use it to extract features from the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c75ETXsLAh1"
      },
      "source": [
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_zz0PWQLdlf"
      },
      "source": [
        "We'll make a *new* model out of the \"headless\" already-fitted MobileNetV2, with a brand-new, totally untrained classification head on top:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBnKy9DZLqKp"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(base_model)\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=3,\n",
        "    activation=tf.keras.activations.softmax\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkfSks3nL4hW"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6OfB4vEMP5R"
      },
      "source": [
        "We'll compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcblgltBMRKm"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg4RvRbCNDHP"
      },
      "source": [
        "Also, we'll use data augmentation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR7na5vJNIRz"
      },
      "source": [
        "BATCH_SIZE=256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjQR3RNGNE3M"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
        "                         height_shift_range=0.08, zoom_range=0.08)\n",
        "train_generator = train_gen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_gen = ImageDataGenerator()\n",
        "val_generator = val_gen.flow(X_test, y_test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7efX5BhMbPr"
      },
      "source": [
        "Now we can start training our model. Remember, we are *only* updating the weights in the classification head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK_HisYmMfN5"
      },
      "source": [
        "n_epochs = 20\n",
        "\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=n_epochs,\n",
        "    steps_per_epoch=X_train.shape[0]//BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=X_test.shape[0]//BATCH_SIZE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FjtsMm0RkEq"
      },
      "source": [
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "accuracy = hist.history['accuracy']\n",
        "val_accuracy = hist.history['val_accuracy']\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(loss, label='Training set')\n",
        "plt.plot(val_loss, label='Test set', linestyle='--')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(accuracy, label='Training set')\n",
        "plt.plot(val_accuracy, label='Test set', linestyle='--')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x032z3Q7Xt6J"
      },
      "source": [
        "## Fine-tune model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAA-ZvXZXvI_"
      },
      "source": [
        "We have fitted our own classification head, but there's one more step we can attempt to customize the model for our particular application.\n",
        "\n",
        "We are going to \"un-freeze\" the later parts of the model, and train it for a few more epochs on our data, so that the high-level features are better suited for our specific classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uaocqRSX968"
      },
      "source": [
        "base_model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhTRFkNSX-si"
      },
      "source": [
        "len(base_model.layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7RoeI_2YhU-"
      },
      "source": [
        "Note that we are *not* creating a new model. We're just going to continue training the model we already started training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYhwL1tLYK_N"
      },
      "source": [
        "fine_tune_at = 149\n",
        "\n",
        "# freeze first layers\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False\n",
        "\n",
        "# use a smaller training rate for fine-tuning\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
        "model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYhTre83Ydc8"
      },
      "source": [
        "n_epochs_fine = 20\n",
        "\n",
        "hist_fine = model.fit(\n",
        "    train_generator,\n",
        "    epochs=n_epochs + n_epochs_fine,\n",
        "    initial_epoch=n_epochs,\n",
        "    steps_per_epoch=X_train.shape[0]//BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=X_test.shape[0]//BATCH_SIZE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cinBoEVYw88"
      },
      "source": [
        "loss = hist.history['loss'] + hist_fine.history['loss']\n",
        "val_loss = hist.history['val_loss'] + hist_fine.history['val_loss']\n",
        "\n",
        "accuracy = hist.history['accuracy'] + hist_fine.history['accuracy']\n",
        "val_accuracy = hist.history['val_accuracy'] + hist_fine.history['val_accuracy']\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(loss, label='Training set')\n",
        "plt.plot(val_loss, label='Test set', linestyle='--')\n",
        "plt.plot([n_epochs, n_epochs], plt.ylim(),label='Fine Tuning',linestyle='dotted')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(accuracy, label='Training set')\n",
        "plt.plot(val_accuracy, label='Test set', linestyle='dotted')\n",
        "plt.plot([n_epochs, n_epochs], plt.ylim(), label='Fine Tuning', linestyle='--')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZpP4OMkZn3n"
      },
      "source": [
        "## Classify custom test sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP6SVyxBZp8p"
      },
      "source": [
        "test_probs = model.predict(test_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVhrR9kDZvwX"
      },
      "source": [
        "plt.figure(figsize=(10,4));\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(test_sample.reshape(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3));\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "p = sns.barplot(x=classes,y=test_probs.squeeze());\n",
        "plt.ylabel(\"Probability\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5X0V8OjfFXS"
      },
      "source": [
        "## Some comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdRKMpL1tztv"
      },
      "source": [
        "In practice, for most machine learning problems, you wouldn't design or train a convolutional neural network from scratch - you would use an existing model that suits your needs (does well on ImageNet, size is right) and fine-tune it on your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E2Y5BStfGi6"
      },
      "source": [
        "Transfer learning isn't only for image classification.\n",
        "\n",
        "There are many problems that can be solved by taking a VERY LARGE task-generic \"feature detection\" model trained on a LOT of data, and fine-tuning it on a small custom dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWstZ84jfTEc"
      },
      "source": [
        "For example, consider [AI Dungeon](https://play.aidungeon.io/), a game in the style of classic text-based adventure games.\n",
        "\n",
        "It was trained by fine-tuning a version of GPT-2. GPT-2 is a language model with 1.5 billion parameters, trained on a dataset of 8 million web pages, with the objective of predicting the next word in a sequence.\n",
        "\n",
        "The creator of AI Dungeon fine-tuned GTP-2 using story-games scraped from [ChooseYourStory.com](https://chooseyourstory.com/Stories/).\n",
        "\n",
        "With so much data, the model doesn't only learn about language and language features - it learns about the world described by all that text! Since the game is based on a fine-tuned version of that model, it also knows a lot about the world.\n"
      ]
    }
  ]
}